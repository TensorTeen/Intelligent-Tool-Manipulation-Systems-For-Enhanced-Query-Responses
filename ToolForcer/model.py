
from lmformatenforcer import JsonSchemaParser, CharacterLevelParser, RegexParser, StringParser
from lmformatenforcer.integrations.transformers import generate_enforced
from lmformatenforcer.integrations.vllm import build_vllm_logits_processor
from vllm import SamplingParams
import vllm
from transformers import AutoModelForCausalLM, AutoTokenizer
import pandas as pd
from typing import Tuple, Optional, Union, List
from awq import AutoAWQForCausalLM
StringOrManyStrings = Union[str, List[str]]
ListOrStrList = Union[str, List[str]]

class Model():
    def __init__(self, model_id, device, load_in_4bit=True, use_vllm=True):
        """
        Initializes a new instance of the class.
        
        Args:
            model_id (str): The ID of the model to be used.
            device (str): The device to run the model on.
            load_in_4bit (bool, optional): Whether to load the model in 4-bit mode. Defaults to True.
            use_vllm (bool, optional): Whether to use VLLM. Defaults to True.
        """
        self.use_vllm=use_vllm
        if use_vllm:
            if load_in_4bit:
                self.model=vllm.LLM(model=model_id, quantization="AWQ", dtype="float16")
            else:
                self.model=vllm.LLM(model=model_id, trust_remote_code=True)
        else:
            self.model=AutoModelForCausalLM.from_pretrained(model_id, device_map='auto', load_in_4bit=load_in_4bit)
        self.tokenizer=AutoTokenizer.from_pretrained(model_id)
        self.device=device
        if self.tokenizer.pad_token_id is None:
            # Required for batching example
            self.tokenizer.pad_token_id = self.tokenizer.eos_token_id 

    def vllm_with_character_level_parser(self, llm: vllm.LLM, prompt: ListOrStrList, parser: Optional[CharacterLevelParser] = None, max_new_tokens=1500) -> ListOrStrList:
        """
        Generates a list of text outputs using a character-level parser with a variable-length language model (VLLM).

        Args:
            llm (vllm.LLM): The variable-length language model.
            prompt (ListOrStrList): The input prompt for generating text outputs.
            parser (Optional[CharacterLevelParser]): The character-level parser to process the logits. Default is None.
            max_new_tokens (int): The maximum number of new tokens to generate. Default is 1500.

        Returns:
            ListOrStrList: A list of text outputs generated by the VLLM. If the input prompt is a string, a single text output is returned as a string. If the input prompt is a list, multiple text outputs are returned as a list of strings.
        """
        
        sampling_params = SamplingParams()
        sampling_params.max_tokens = max_new_tokens
        if parser:
            logits_processor = build_vllm_logits_processor(llm, parser)
            sampling_params.logits_processors = [logits_processor]
            
        results = llm.generate(prompt, sampling_params=sampling_params)
        if isinstance(prompt, str):
            return results[0].outputs[0].text
        else:
            return [result.outputs[0].text for result in results]
            
    
    def run(self,
            message: StringOrManyStrings,
            system_prompt: str,
            max_new_tokens: int = 1024,
            temperature: float = 0.8,
            top_p: float = 0.95,
            top_k: int = 50,
            num_beams: int = 2,
            required_regex: Optional[str] = None,
            required_str: Optional[str] = None,
            required_json_schema: Optional[dict] = None,
            required_json_output: Optional[bool] = None) -> Tuple[StringOrManyStrings, Optional[pd.DataFrame]]:
        """
        Runs the model on the given input message(s) and generates text based on the system prompt.

        Args:
            message (StringOrManyStrings): The input message(s) to generate text for. It can be a single string or a list of strings.
            system_prompt (str): The system prompt to use for text generation.
            max_new_tokens (int): The maximum number of new tokens to generate.
            temperature (float): The temperature value for text generation. Higher values (e.g., 1.0) make the output more random, while lower values (e.g., 0.2) make it more focused and deterministic.
            top_p (float): The nucleus sampling parameter for text generation. It selects the highest probability tokens whose cumulative probability exceeds this value.
            top_k (int): The top-k sampling parameter for text generation. It restricts the sampling to the top k tokens with the highest probabilities.
            num_beams (int): The number of beams to use for beam search during text generation.
            required_regex (Optional[str]): The regex pattern that the generated text must match.
            required_str (Optional[str]): The exact string that the generated text must match.
            required_json_schema (Optional[dict]): The JSON schema that the generated JSON output must match.
            required_json_output (Optional[bool]): Whether the generated output must be valid JSON.

        Returns:
            Tuple[StringOrManyStrings, Optional[pd.DataFrame]]: A tuple containing the generated text(s) and an optional DataFrame of enforced scores (if a parser is provided).

        Note:
            - If `message` is a list, the function generates text for each message in the list.
            - The function uses the `system_prompt` to guide text generation.
            - The generated text(s) are returned as a list if `message` is a list, otherwise a single string is returned.
            - If a parser is provided (regex, string, JSON schema), the function enforces the generated text to match the specified pattern or schema.
            - If a parser is provided and `message` is not a list, the function returns an optional DataFrame of enforced scores.

        Example:
            run("Hello", "System prompt", max_new_tokens=100, temperature=0.8)
        """
        
        
        is_multi_message = isinstance(message, list)
        messages = message if is_multi_message else [message]
        prompts = [self.get_prompt(msg, system_prompt) for msg in messages]
        inputs = self.tokenizer(prompts, return_tensors='pt', add_special_tokens=False, return_token_type_ids=False, padding=is_multi_message).to(self.device)

        generate_kwargs = dict(
            inputs,
            # streamer=streamer,
            max_new_tokens=max_new_tokens,
            do_sample=True,
            top_p=top_p,
            top_k=top_k,
            temperature=temperature,
            num_beams=num_beams,
            output_scores=True,
            return_dict_in_generate=True,
        )

        parser: Optional[CharacterLevelParser] = None
        if required_regex:
            parser = RegexParser(required_regex)
        if required_str:
            parser = StringParser(required_str)
        if required_json_schema:
            parser = JsonSchemaParser(required_json_schema)
        if required_json_output:
            parser = JsonSchemaParser(None)

        if parser:
            output = generate_enforced(self.model, self.tokenizer, parser, **generate_kwargs)
        else:
            output = self.model.generate(**generate_kwargs)

        sequences = output['sequences']
        # skip_prompt=True doesn't work consistenly, so we hack around it.
        string_outputs = [self.tokenizer.decode(sequence, skip_special_tokens=True) for sequence in sequences]
        string_outputs = [string_output.replace(prompt[3:], '') for string_output, prompt in zip(string_outputs, prompts)]
        if parser and not is_multi_message:
            enforced_scores_dict = output.enforced_scores
            enforced_scores = pd.DataFrame(enforced_scores_dict)
            pd.set_option('display.width', 1000)
            pd.set_option('display.max_columns', 10)
            pd.set_option('display.max_rows', 999)
            pd.set_option('display.float_format', ' {:,.5f}'.format)
        else:
            enforced_scores = None

        return string_outputs if is_multi_message else string_outputs[0] #, enforced_scores

    def __call__(self, message, system_prompt, required_json_schema, max_new_tokens=1500):
        if self.use_vllm:
            prompt = self.get_prompt(message, system_prompt)
            output = self.vllm_with_character_level_parser(self.model, prompt, JsonSchemaParser(required_json_schema), max_new_tokens)
        else:
            output = self.run(message=message, system_prompt=system_prompt, max_new_tokens=max_new_tokens, required_json_schema=required_json_schema)

        return output
            

    
    def get_prompt(self, message: str, system_prompt: str) -> str:
        prompt = f"GPT4 User: {system_prompt}\n\n{message}<|end_of_turn|>GPT4 Assistant:"

        return prompt
